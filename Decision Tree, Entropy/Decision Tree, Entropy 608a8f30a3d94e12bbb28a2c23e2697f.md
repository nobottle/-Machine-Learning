# Decision Tree, Entropy

의사결정트리라고도 불림, 분류(classification)와 회귀(regression)분석에 사용되는 지도 학습 알고리즘 중 하나, 이진 분할(binary split)을 통해 데이터를 분할하고 판단하는 모델

일련의 결정 규칙을 나타내는 트리 구조를 사용 함

트리 꼭대기에서 시작하여 나눠지는 분기점에서  데이터의 특정 특성에 따라 두 개 이상의 하위 그룹으로 분할

각 분할마다 최적의 특성과 기준을 찾기 위한 알고리즘을 사용

이 규칙을 학습하고 나무구조를 만드는 것이 모델의 학습과정임

그런데 과적합발생,데이터의 변화에 민감한 단점을 가짐

---

**Entropy**

어떤 시스템 내부의 무질서함 혹은 불확실성을 나타내는 척도

정보이론에서는 엔트로피를 확률 변수의 분포를 기반으로 계산. 엔트로피의 계산 방식은 가능한 모든 상태에 대한 정보량을 평균화 하는 것,이는 불확실성 정도를 측정하는 척도로 사용

확률변수의 분포가 동일할 경우(모든 결과가 동일한 확률로 발생할 때)엔트로피는 최대 값을 가짐, 분포가 어느 한 결과에 치우쳐 있을 경우(한 결과에 대한 확률이 높을 때) 엔트로피는 최소 값

엔트로피가 최소 값?? >>불확실성이 낮다

항상 확률이 반이면 예측하기가 어렵다 나오는 것을 >>불확실성 큼 엔트로피 최대

근데 어떠한 기준으로 많은 확률을 보인 상황이 있다 이 때는 엔트로피가 최소 불확실성이 낮기 때문에

나누는 기준의 숫자나 나눈것들의 그 나누는 기준의 수가 비슷하면 불확실성이 높다

![스크린샷 2023-04-03 오전 11.58.06.png](Decision%20Tree,%20Entropy%20608a8f30a3d94e12bbb28a2c23e2697f/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-03_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_11.58.06.png)

H(X) : 엔트로피

P(xi) : xi이벤트가 발생할 확률

n : 이벤트의 개수

엔트로피는 각각의 이벤트가 발생할 확률과 이 이벤트의 정보량(loeg2p(xi))을 곱한 값들을 모두 합한 것의 부호를 바꾼 값!!

log2p(xi)에 대해 예를 들자면 동전 앞,뒤 확률이 0.5라고 하면 앞뒤든 나올 수 있는 정보의 양은 같으므로 -log2p(xi) 값은 -log2(0.5)=1 이다

근데 앞면이 0.9 뒷면이 0.1 이면 앞면이 나왓을 때 엎을 수 있는 정보의 양은 매우 적음 이 경우는 log2P(xi) 값은 -log2(0.9) = 0.15

즉, 이벤트가 발생할 확률이 낮을수록 해당 이벤트가 발생했을 때 얻을 수 있는 정보의 양이 많아진다  따라서, 이 값은 해당 이벤트가 가지는 정보의 중요도를 나타내는 척도로 사용됨!!!

**조건부 엔트로피(Conditional entropy)**

확률 변수 X Y가 있을 때 X의 값을 알고 있을 때 Y의 불확실성을 측정 하는 것, 즉 Y가 X에 대해 얼마나 예측 가능한지를 측정

H(Y|X) = -Σ p(y|x) * log p(y|x)

p(y|x)는 X가 주어졌을 때 Y가 발생할 확률을 나타냄

X가 주어졌을 때 Y의 불확실성을 나타내는데, 이 값이 작을수록 Y가 X에 대해 예측 가능하다는 것을 의미

ex)

날씨가 맑은 날에는 축구 경기가 열릴 확률이 높고, 비가 오는 날에는 축구 경기가 열릴 확률이 낮을 수 있슴

따라서 날씨가 맑은 날에는 축구 경기가 열릴 확률을 예측하기 쉬우므로, 조건부 엔트로피는 **작을 것**

**Information gain(정보 이득)**

Information Gain(S, A) = H(S) - H(S|A)

여기서 H(S)는 S 집합의 엔트로피를 나타내며, H(S|A)는 A 속성을 이용하여 S를 분할한 후, 각 분할된 부분 집합에서의 엔트로피 평균값

이렇게 Information gain을 좀 더 간단하게 표현하면, 어떤 속성이 분류에 가장 유용한지 쉽게 판단할 수 있당 즉, Information gain이 높은 속성을 선택하면, 더 정확한 분류 모델을 만들 수 있슴

![KakaoTalk_Photo_2023-04-03-13-25-02.jpeg](Decision%20Tree,%20Entropy%20608a8f30a3d94e12bbb28a2c23e2697f/KakaoTalk_Photo_2023-04-03-13-25-02.jpeg)

간단히 만들어 본건데 데이터 전처리나 추가적인 조건 등을 추가하여 의사결정트리를 수정해야 함 위처럼 오답이 나올수도 잇기 때문