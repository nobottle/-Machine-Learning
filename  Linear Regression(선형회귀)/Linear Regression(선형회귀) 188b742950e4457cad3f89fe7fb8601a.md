# Linear Regression(선형회귀)

머신러닝에서 가장 큰 목적은 실제 데이터를 바탕으로 모델을 생성 >>so 다른 입력값을 넣었을 때 아웃풋을 예측하는 거

이때 가장 직관적이고 간단하게는 선line으로 표현하는 거 그래서 데이터를 놓고 그거를 잘 설명할 수 있는 선을 찾는 분석하는 법을 선형회귀라고 함

![스크린샷 2023-04-05 오후 3.15.45.png](Linear%20Regression(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20188b742950e4457cad3f89fe7fb8601a/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-05_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.15.45.png)

키랑 몸무게를 잘 예측할 수 있는 초록색 선을 그어놓고 예측하는 거

정확하지는 않지만 비슷하다

y=Wx+b 이렇게 대략 식이 나오는데

선형회귀의 목적은 최적은 m과 b를 찾는 것

![스크린샷 2023-04-05 오후 3.17.16.png](Linear%20Regression(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20188b742950e4457cad3f89fe7fb8601a/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-05_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.17.16.png)

A와 B는 데이터라고 치자

이 때 선에서 A,B는 3,1만큼 수치의 오차가 발생

이때 이 손실을 선형회귀에서는 제곱을 만든다

즉 9,1만큼의 손실이 있다고 생각하는 거 >>이게 바로 평균 제곱 오차(mean squarred error)

결국 선형 회귀 모델은 모든 데이터로부터 나타나는 이 오차의 평균들을 최소화 할 수 있는 기울기 (위에서는 w)과 절편 (위에서는 b)를 ㅊ자는 것!!!

손실을 최소화 하기 위해 공부한 경사하강법이 나옴

![스크린샷 2023-04-05 오후 3.23.38.png](Linear%20Regression(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20188b742950e4457cad3f89fe7fb8601a/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-05_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.23.38.png)

절편을 구할 때 사용하는 공식이다

N은 데이터의 개수일거고

![스크린샷 2023-04-05 오후 3.24.21.png](Linear%20Regression(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20188b742950e4457cad3f89fe7fb8601a/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-05_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.24.21.png)

이거는 기울기를 구하는 공식

좀만 더 제대로 알아보면 

경사하강법으로 최적의 알고리즘 최소 손실을찾는 알고리즘을 옵티마이저 또는 최적화 알고리즘이라고 함

그래서 이 옵티마이저를 통해 w,b를 찾아내는 과정을 머신러닝에서 훈련,학습이라고 함

일단 b를 생각 안하고 w만 생각해서 y=wx라는 선형회귀가 있다고 하자

비용함수의 값은 Cost라고 하자

![스크린샷 2023-04-05 오후 3.28.54.png](Linear%20Regression(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20188b742950e4457cad3f89fe7fb8601a/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-05_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.28.54.png)

w가 커지면 cost또한 커진다

기울기인 W가 커지면 cost는 무한대로 커지고 반대로 기울기가 작아도 cost값은 무한대로 커진다 이때 저기 볼록한 부분 cost가 제일 작을 때인데 여기값을 가지게하는 W를 찾아야 한당

![스크린샷 2023-04-05 오후 3.30.55.png](Linear%20Regression(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20188b742950e4457cad3f89fe7fb8601a/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-05_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.30.55.png)

이런식으로 기울기마다 거기 위치를 보여주는데 기울기가 0이되는 부분이 저기 최소볼록한 부분이다

즉 cost가 최소화 되는 지점은 접선의 기울기가 0이 되는 지점,미분값이 0이되는 지점데쓰

그럼 다시 돌아가 비용함수는 

![스크린샷 2023-04-05 오후 3.23.38.png](Linear%20Regression(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20188b742950e4457cad3f89fe7fb8601a/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-05_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.23.38.png)

m을 x로 생각하면 됨

wx+b가 원래 우리가 생각한 선이고 이거의 y값은 예측한 값인데 

yi는 진짜 예측값 yi-wx+b가 오차 이 값을 찾는거!

그래서 이식이 나옴

그래서 원래 w에서 미분한거를 계쏙 빼주는 작업

현재 w에서의 접선의 기울기와 a와 곱한 값을 현재 W에서 빼서 새로운 w의 값으로 한다 a는 학습률 기울기가 0이될 때 까지 반복함

![스크린샷 2023-04-05 오후 3.57.58.png](Linear%20Regression(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20188b742950e4457cad3f89fe7fb8601a/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-05_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.57.58.png)

그렇다면 선형회귀는 언제 왜 쓰는가?

선형 회귀는 주어진 데이터에서 독립 변수와 종속 변수 사이의 선형적인 관계를 모델링하는 데 사용됨. 이를 통해 독립 변수가 종속 변수에 미치는 영향을 예측하고, 새로운 독립 변수 값에 대한 종속 변수 값을 예측할 수 있다

예를 들어, 경제학에서는 소비자 지출, 투자, 수입 등의 요인들과 GDP와의 관계를 분석하는 데에 사용. 의학에서는 연령, 성별, 혈압 등과 같은 요인들과 질병 발생의 관계를 분석하는 데에 사용

회귀 분석은 독립 변수와 종속 변수 사이의 관계를 분석하는 통계적 방법 회귀 분석은 종속 변수와 독립 변수 간의 관계를 모델링하고, 이를 통해 독립 변수가 종속 변수에 미치는 영향력을 파악 회귀 분석은 예측, 인과관계 파악, 영향력 분석 등 다양한 분야에서 사용됨

뭐 이렇다고 함