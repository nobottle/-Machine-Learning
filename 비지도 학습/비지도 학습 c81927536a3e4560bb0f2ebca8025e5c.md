# 비지도 학습

Clustering(군집화) : 비슷한 샘플을 하나의 cluster 또는 비슷한 샘플의 그룹으로 할당하는 작업

![스크린샷 2023-05-24 오후 1.25.11.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.25.11.png)

꽃의 데이터셋들을 모아놓은 건데, 왼쪽은 레이블이 있고 오른쪽은 레이블이 없다

오른쪽과 같이 레이블이 없는 데이터셋은 분류와 같은 지도 학습 알고리즘을 적용할 수 없음

>>레이블이 없기 때문에 뭐가 뭔지 잘 모른다

즉, 위 그림의 데이터셋에서 `petal length`, `petal with` 특성만을 추출하여 평면상에 나타낸 것인데, 왼쪽(분류, 지도학습)에서는 각 점들이 어떤 꽃인지 레이블이 표시되어있다 그러나 오른쪽(군집, 비지도학습)에는 각 샘플의 특성만 알 수 있고 어떤 꽃인지는 알 수 없다.

****k-means 알고리즘****

주어진 데이터를 k개의 cluster로 묶는 알고리즘을 말함

방식 :

1. 초기화 : 주어진 데이터셋에서 무작위로 k개의 샘플을 뽑아 centroid로 지정
2. 거리계산 : 모든 샘플에 대해 각 centroid와의 거리를 계산
3. 클러스터 할당 : 각 샘플들은 가장 가까운 거리의 centroid와 하나의 cluster로 묶임
4. centroid업데이트 : 모든 샘플들의 cluster가 정해지면, 각 cluster에 속한 샘플들의 평균값을 성분으로 갖는 새로운 centroid를 지정
5. centroid가 더이상 바뀌지 않을 때 까지 2~4를 계속 반복!

좋은 그림을 가져옴 이해하기가 좀 쉬울듯

![스크린샷 2023-05-24 오후 1.34.25.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.34.25.png)

![스크린샷 2023-05-24 오후 1.39.28.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.39.28.png)

밑에 그림이 점차적으로 훈련되는 과정이라고 함

>>계속해서 centroid가 변하면서 cluster의 형태들이 바뀌고 정돈되고 있다

**다양한 수렴형태**

![스크린샷 2023-05-24 오후 1.41.46.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.41.46.png)

이런식으로 수렴형태들이 다양함 이유는 이제 초기의 centroid가 무작위로 정해지기 때문임

개선 방법은 2가지가 있음

1.반복

간단하게 알고리즘을 존나 반복하여 안좋은 거를 피하는 거, 알고리즘의 반복 횟수를 조절하면서 즉 예를들어 10번을 했다고 치면, 10번동안의 가장 좋은 모델을 선택하면 됨

성능의 평가 기준은? **inertia(이니셔**)로 함

**이너셔(inertia)** : 데이터셋의 각 샘플에 대해 자신이 속한 클러스터의 센트로이드 까지의 거리를 제곱해서 더한 값

이거의 수식은…?

![스크린샷 2023-05-24 오후 1.44.32.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.44.32.png)

x는 데이터 샘플일거고,,,

10번의 훈련을 한다고 가정하면 10번의 훈련 중 이너셔가 가장 작은 모델이 생성, 또한 k-평균 알고리즘은 이너셔가 더이상 감소하지 않는 지점에서 종료

2.k-means++

초기 centroid를 무작위로 고르지 않고 신중하게 고르는 방법

1. 데이터셋에서 무작위로 하나의 centroid인 C를 선택.
2. 각 샘플 x에 대해 확률을 계산
    
    ![스크린샷 2023-05-24 오후 1.48.10.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.48.10.png)
    
    요거…..
    
    D(x(i)) 는 샘플 x(i)와 가장 가까운 centroid까지의 거리 이 알고리즘은 초기 k개의 centroid를 고를 때 서로 멀리 떨어진 샘플이 선택될 확률을 높임!!
    
    >>그래야지 cluster가 명확하게 될 확률이 높아지기 때문
    

****최적의 클러스터 수 k 찾기****

![스크린샷 2023-05-24 오후 1.53.00.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.53.00.png)

k가 3,8일 때의 차이를 보여주는 그림

둘 다 잘못된 k값으로 훈련하면 위 그림처럼 제대로 군집화가 안됨

적절한 k값을 찾는 방법은 2가지임

1.엘보

이니셔의 감소폭이 급감하는 지점을 엘보라고 함

![스크린샷 2023-05-24 오후 1.56.15.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.56.15.png)

연속적인 k값을 택하여 훈련을 갈긴 뒤 이니셔의 감소율을 계산하여 k값을 선택하면 됨 위 그림에서는 3에서 4될때 존나 감소 4가 elbow지점

2.실루엣 점수(silhouette score)

엘보 찾는 것보다 많은 계산을 필요로 하지만 정확한 결과를 얻기 가능

실루엣 점수 : 모든 샘플에 대한 실루엣 계수의 평균

실루엣 계수 = (b-a) / max(a,b)

a : 같은 클러스터에 속한 다른 샘플들 까지의 평균 거리

>>클러스터 내부의 평균거리 즉 데이터의 응집도라고 보면 됨

>>값이 크면? 응집도 떨어짐, 값이 작으면 응집도 ㅈ됨

b : 가장 가까운 외부 클러스터에 속한 샘플들까지의 평균 거리, 데이터의 분리도를 나타냄

>>값이 커야 분리가 잘 되어 있는 거

a가 작을수록 하나의 클러스터 안에 테이터들의 밀도가 높으며, b가 클수록 클러스터들 사이에 빈 공간이 많다

얘네들은 이러한 값들을 가짐

![스크린샷 2023-05-24 오후 2.04.08.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.04.08.png)

따라서 실루엣 계수는 -1부터 1사이의 값 1에 가까울수록 자신의 클러스터와 잘 속해있고 0에 가까울수록 서로 다른 클러스터의 경계에 위치하며, -1에 가까울수록 잘못된 클러스터에 할당되었다는 의미

그러나 실루엣 점수의 최댓값이 가장 이상적인 클러스터링을 보장하지는 않음

![스크린샷 2023-05-24 오후 2.13.34.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.13.34.png)

이렇게 각 k마다의 클러스터한거를 다이어그램으로 나타낸 것

칼모양의 크래프로 나타남, 그래프의 높이는 클러스터에 속한 샘플의 갯수, 너비는 각 샘플의 실루엣 계수

점선은 실루엣 점수 내림차순으로 정렬해서 칼모양임

k=4 일때 실루엣 점수가 가장 높긴 하나, k=5일 때 클러스터의 높이가 가장 균일하며 클러스터별 실루엣 계수들의 편차가 가장 적다. 즉적최적의 k 값은 k=5

**k-means의 한계**

1. 최적이 아닌 솔루션을 피하기 위해 알고리즘을 존나기 실행
2. 클러스터의 개수를 직접 지정
3. 클러스터가 원형이 아닌 경우는 잘 작동하지 않는다고 함

**k-means의 예시**

1.image segmentation(이미지 분할)

이미지 분할의 예시 중 하나인 color segmentation에 대해 알아보자

![스크린샷 2023-05-24 오후 2.24.57.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.24.57.png)

원래 이딴 이미지가 있다고 하자

R,G,B 3차원으로 스타뜨 ㄱㄱ

이 색깔들을 각 k개의 색으로 군집화하면..

![스크린샷 2023-05-24 오후 2.28.23.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.28.23.png)

이런식으로 됨…

**DBSCAN**

밀도 기반 클러스터링임

밀집된 연속적 지역을 클러스터로 정의를 함

작동방식

1. 알고리즘이 각 샘플에서 작은 거리인 ε(하이퍼 파라미터) 내에 샘플이 몇 개 놓여 있는지 개수를 셈

>>이 지역을  ε-이웃 이라고 부름

1. 자기 자신을 포함해 ε-이웃내에 적어도 min_samples (하이퍼 파라미터)개의 샘플이 있다면 이를 **핵심 샘플(core instance)**로 간주

>>핵심 샘플은 밀집된 지역에 있는 샘플

1. 핵심 샘플의 이웃에 있는 모든 샘플은 동일한 클러스터로 속함, 이웃에는 다른 핵심 샘플이 포함될 수 있음. 따라서 샘플의 이웃의 이웃은 계속해서 하나의 클러스터를 형성
2. 핵심 샘플이 아니고 이웃도 아닌 샘플은 이상치로 판단함

이 알고리즘은 모든 클러스터가 충분히 밀집되어 있고 밀집되지 않은 지역과 잘 구분될 때 좋은 성능을 냄

![스크린샷 2023-05-24 오후 2.33.03.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.33.03.png)

그림으로 표현하면…

![스크린샷 2023-05-24 오후 2.49.09.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.49.09.png)

이거를 보면 좀 더 시각적으로 이해하기가 쉬움

dbscan의 특징에서 설명한것과 같이 이 알고리즘은 새로운 샘플에 대해 예측할 수 없다

장점 :

1. k-means와 같이 클러스터 갯수 정하지 않아도 됨
2. 클러스터의 밀도에 따라 클러스터를 연결하기 때문에 ㅈ같은 모양을 갖는 군집도 잘 찾음
3. noise point를 통해 outlier검출 가능

**가우시안 혼합 모델(GMM)**

샘플이 파라미터가 알려지지 않은 여러 개의 혼합된 가우시안 분포에서 생성되었다고 가정하는 확률 모델

하나의 가우시안 분포에서 생성된 샘플은 하나의 클러스터를 형성

일반적으로 타원형

![스크린샷 2023-05-24 오후 2.53.24.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.53.24.png)

대략 그림으로는 이런식일 듯

GMM은 사전에 가우싣안 분포의 개수 k를 알아야 함

![스크린샷 2023-05-24 오후 2.55.44.png](%E1%84%87%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%83%E1%85%A9%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c81927536a3e4560bb0f2ebca8025e5c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.55.44.png)

이게 좀 정리가 된거…

내 수준에서 지금 쉽게 정리해보자면

데이터들이 있다.

처음에 랜덤하게 가우시안 분포를 무작위로 그림

데이터들간의 최대모수를 찾는 과정으로 계속해서 분포를 바꿔나감