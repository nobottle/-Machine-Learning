# Linear Regression

선형회귀란?

독립변수가 종속변수에 미치는 영향력의 크기를 파악, 독립변수의 일정한 값에 대응하는 종속변수 값을 예측하는 모형을 산출하는 방법

![스크린샷 2023-04-30 오후 4.17.32.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-30_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.17.32.png)

linear regression 그림

독립변수 : 결과에 영향을 주는 변수 (설명변수)라고도 함 위 그림에서는 x축이라고 보면 됨

종속변수 : 결과를 나타내는 변수, 종속변수는 독립변수에 의해 영향을 받는 변수이며, 예측하고자 하는 대상

위 그림에서는   y축이라고 보면 됨

**단순선형회귀분석(simple Linear Regression Analysis)**

y=ax+b

독립변수 x와 곱해지는 값w를 머신러닝에서는 가중치(weight), 별도로 더해지는 b이거를 bias라고 함

**다중 선형 회귀 분석 (Muktiple Linear Regression Analysis)**

![스크린샷 2023-04-30 오후 4.20.27.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-30_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.20.27.png)

여기서 오른쪽 밑 숫자들은 데이터의 개수들인데 1,2,….n개의 데이터가 있다고 보면 됨

만약 예를 들어서

![캡처.PNG](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25EC%25BA%25A1%25EC%25B2%2598.png)

이런 표가 있다고 가정했을 때

x,y축으로 만들 수있고 그렇다면 x인 hours로부터 y인 score의 대략적인 관계 분석이 가능하다

여기서 수학식을 만들 수 있는데 이를 가설(hypothesis)라고 함

그렇다면 이 데이터를 가장 잘 나타낼 수 있는 수학 식 wx+b를 그리는게 목표임

즉 제일 데이터들이랑 들어맞는 w와 b를 찾는게 linear regresssion의 목표

**비용함수(cost function)**

실제 값 y가 있고 우리가 예측한 값을 yhat이라고 햇을 때 이 차이를 최소화하는 걸 찾아야 하는데 이 예측값에 대한 오차의 식을 목적함수 OR 비용함수OR손실함수 라고 함

회귀문제에서는 주로 MSE(평균 제곱 오차)를 사용

![F.PNG](Linear%20Regression%20b045f668fd17450894082aece2ad4110/F.png)

임의의 선을 그었다고 치자 저 빨간색 화살표가 데이터들과의 거리인데 모든 데이터들과의 거리가 적은 최대의 직선을 그리는게 목표

MSE식은 일단

![DDD.PNG](Linear%20Regression%20b045f668fd17450894082aece2ad4110/DDD.png)

N은 갯수

y는 real정답

h는 내가구한 답

이걸 재 정의하면 

![dddddddddd.PNG](Linear%20Regression%20b045f668fd17450894082aece2ad4110/dddddddddd.png)

저 cost를 min하는걸 찾는게 목표

최소화하는 과정에는 두 가지가 있는데 정규방정식과 경사하강법 두 가지를 이용해 계산이 가능하다

**정규방정식**

정규방정식은 최소자승법(least squares method)을 이용하여 회귀분석(regression analysis)을 수행할 때 사용되는 방정식이라고 함

![스크린샷 2023-05-01 오전 12.53.57.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-01_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_12.53.57.png)

이렇게 됐다고 해보자

저기 cost 1/2m이 있는데 왜 갑자기 2가 생겻나? ex) (x-y)**2를 미분한다치면2(x-y) <x에대해> 이렇게 결과가 나오는데 2를 없애서 편하게 해주기 위함 즉 계산을 편리하게끔 하기 위한 것

ㅅㅂ,,이거를 이해하고 찾는데 몇시간을 썻나 후,,,

그렇다면 우리는 a,b의 최솟값을 찾는게 목표이므로 각각에 대해 편미분을 갈겨준다

![스크린샷 2023-05-01 오전 12.56.47.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-01_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_12.56.47.png)

이렇게 됨 왜 이렇게 되냐고?

정리해봄

![KakaoTalk_Photo_2023-05-01-01-11-30.jpeg](Linear%20Regression%20b045f668fd17450894082aece2ad4110/KakaoTalk_Photo_2023-05-01-01-11-30.jpeg)

ㅇㅋ

![스크린샷 2023-05-01 오전 1.13.32.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-01_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_1.13.32.png)

그래서 식을 정리했을 때 이렇게 됨

그래서 이거를 행렬로 나타내면

![스크린샷 2023-05-01 오전 1.12.08.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-01_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_1.12.08.png)

m을 놓은 이유는 상수b를 시그마 했을 때 그냥 m만큼 더한 값을 곱한거 이므로 저렇게 표현

이거를 간단히 하면?

![스크린샷 2023-05-01 오전 1.14.20.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-01_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_1.14.20.png)

![스크린샷 2023-05-01 오전 3.07.13.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-01_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_3.07.13.png)

최종은 이렇게 됨

왼오른쪽에 역행렬을 곱해주면 왼쪽은 xtx가 사라지고 오른쪽에 붙으니…

ㅅㅂ,,,이거를 이해하는데 거의하루쓴거같네,

근데 이방법은 계산이 많아지면 시간과 비용이 증가하므로 잘 쓰지는 않음

**경사하강법**

Optimizer

비용함수를 최소화하는 w와 b를 구하는 최적화 알고리즘

![스크린샷 2023-05-01 오전 3.16.03.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-01_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_3.16.03.png)

저기 노란색 부분이 최소부분이므로 저기로 보내는게 목포표임

![스크린샷 2023-05-01 오전 3.16.30.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-01_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_3.16.30.png)

이렇게 있다고 하면 원래 w에서 a는 lr

w에서 a곱한 편미분한 비용을 뺀 w로 업데이트!

a가 너무크면도 안되고 작으면도 안됨

여튼 접선의 기울기가 0에 가까워질수록 Cost를 w로 편미분한게 작아짐

![스크린샷 2023-05-01 오전 3.18.25.png](Linear%20Regression%20b045f668fd17450894082aece2ad4110/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-01_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_3.18.25.png)

이 그림은 lr이 너무커서 발산하는 경우의 그림

근데 b에대해서도 옵티마이징절차를 수행해야하는데…

**ordinary least square(최소자승법) >>이거는 시,,,이해가 너무 안간다**

OLS는 RSS를 최소화하는 가중치 벡터를 행렬 미분으로 구하는 방법