# Logistic Regression,Linear Regression복습

로지스틱 회귀란?

데이터가 어떠한 범주에 속할 확률을 0에서1사이의 값으로 예측하고 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘데쓰

뭐 종이를 받았을 때 그 종이가 돈이 확률이 0.5이상이면 돈으로 분류 그것보다 작으면 쓰레기로 분류한다는 이런식으로 이해하면 됨

여기서는 데이터가2개범주니까 여기세 속하도록 결정하는 것을 binary classification(이진분류)라고 함

그 전 선형회귀 복습

머신러닝의 목적은 데이터를 바탕으로 모델을 생성해 다른 입력값을 넣엇을 때 발생할 아웃풋을 예측하는 것

직관적이고 간단한 모델은 선(line)임

데이터를 놓고 그거를 잘 설명할 수 잇는 선을 찾아 분석하는 방법을 선형화귀(lineart regression)이라고 함

![스크린샷 2023-04-16 오후 3.24.17.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.24.17.png)

이러한 것 키와 몸무게를 바탕으로 예측하는 것

정확하지 않지만 추정은 가능함

y = ax+b

기울기 a,절편b에 따라 그 선의 모양이 정해져서 x를 넣은면 y구하기 가능

즉 우리는 a,b를 잘 구하는 거가 목표!

![스크린샷 2023-04-16 오후 3.28.24.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.28.24.png)

데이터들을 놓고 선을 넣는다는 것 어림잡는 것인데 오타가 무조건 발생함

이를 loss라고 하자

이 오차를 -,+고려하지 않고 그냥 손실에 제곱을 해서 하자

그러면 A는 9, B는 1임

이런 방식으로 손실을 구하는 거를 mean squared error(평균 제곱 오차)(MSE)라고 함

이것도 있고 절대값으로 평균을 구하는 mean absolute error(MAE) 뭐 다른것들도 잇다

즉 선형회귀모델을 모든 데이터로부터 나타나는 오차의 평균을 최소화할수 있는 최적의 기울기와 절편을 찾는 것

일반 머신러닝에서 사용하는 모형은 매우 복잡해, 선형회귀분석에서도 최적의 기울기와 절편을 구하기가 쉽지 않은데 방법은??

![스크린샷 2023-04-16 오후 3.36.09.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.36.09.png)

사진을 보면 자 저기 파라미터를 조금씩 변화시켜 손실을 줄여가는 방법으로 최적의 파라미터를 찾아가는데

경사하강법이라고 부름

절편을 구할 때 공식 :

![스크린샷 2023-04-16 오후 3.37.12.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.37.12.png)

기울기를 구하는 공식 :

![스크린샷 2023-04-16 오후 3.37.32.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.37.32.png)

최소의 b,m이 되는 값을 찾는 과정임

이런 선형회귀분석을하면서 기울기,절편을 계속 변경하면서 최적의 값을 찾아가는데

언제까지 할거?

![스크린샷 2023-04-16 오후 3.38.20.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.38.20.png)

이렇게 시도를 계속 하다보면 어디에 수렴하는 값이 됨 

여기까지 선형회귀 복습

---

![스크린샷 2023-04-16 오후 3.45.56.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.45.56.png)

이거를보면 회귀선을 보면 존나 공부한 시간이 많으면 패스할 가능성이 많아짐

근데 확률이 0,1을 벗어나 음수,1.1막 이런식으로 커짐 이상함

![스크린샷 2023-04-16 오후 3.47.17.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.47.17.png)

근데 로지스틱회귀로 바꾸면 0,1사이로 됨

로지스틱 회귀에서는 데이터가 특정 범주에 속할 확률을 예측하기 위해 아래와 같은 단계를 거침

1. 모든 속성(feature)들의 계수(coefficient)와 절편(intercept)을 `0`으로 초기화한다.
2. 각 속성들의 값(value)에 계수(coefficient)를 곱해서 **log-odds**를 구한다.
3. **log-odds**를 **sigmoid 함수**에 넣어서 `[0,1]` 범위의 확률을 구한다.

Log-Odds란?

선형회귀에서는 각 속성의 값에다가 계수에 각각 곱하고 절편에 더해서 예측값을 구함

ax + b의 형태니까

그래서 구한 예측 값의 범위는 무한대임, 로지스틱 회귀에서는 마지막에 log-odds라는거를 구해준다

odds란?

`odds = 사건이 발생할 확률 / 사건이 발생하지 않을 확률`

임

그래서 만약 학생이 0.7 확률로 시험에 합격한다면, 당연히 시험에서 떨어질 확률은 0.3이 되니까 이렇게 0.7 / 0.3d으로 계산

여기에 log를 씌운것이 lod_odds임

로지스틱회귀에서는 확률을 0,1로 커브모양으러 나타내야 함 이게 바로 sigmoid함

![스크린샷 2023-04-16 오후 5.23.13.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.23.13.png)

위에서 구한 log-odds를 sigmoid함수에 넣어서 0,1사이의 값으로 반환

**로그손실**

로지스틱 회귀가 확률을 제대로 예측해주는지,  구해놓은 속성들의 계수와 절편이 적절한지 확인하기 위해 손실(Loss)을 고려해야 함

**아무튼 모델의 “적합성”을 평가하기 위해 각 데이터 샘플의 손실(모델 예측이 얼마나 잘못되었는지)을 계산한 다음 그것들의 평균화 해야 함**

**로지스틱 회귀에 대한 손실 함수는 Log Loss(로그 손실)**라고 부르며, 아래와 같이 구할 수 잇음

![스크린샷 2023-04-16 오후 5.24.57.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.24.57.png)

목표는 로지스틱 함수를 구성하는 계수와 절편에 대해 log loss를 최소화하는 값을 찾는 것

여기서 손실을 두개로 나눠서 이해할 필요가 잇음

왜냐 2진분류기 때문에

y=1

어떤 데이터 샘플의 클래스 y가 1인경우, 예를들어 학생이 시험에 합격할 경우

![스크린샷 2023-04-16 오후 5.28.01.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.28.01.png)

이 데이터의 손실을 학생이 시험에 통과할 확률에 로그를 씌운 것

y=0

학생이 시험에 탈락한 경우

![스크린샷 2023-04-16 오후 5.28.44.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.28.44.png)

학생이 시험에 탈락할 확률을 1에서 빼고 로그를 씌운 것

결국 분류(레이블)가 **y=1, y=0 일 때 각각의 손실 함수로 나타내면**

![스크린샷 2023-04-16 오후 5.29.23.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.29.23.png)

예측을 잘하면 손실이 적고 잘못하면 많다

그래서 올바른 ㅇ/ㅖ측을 하면서 솔실이 작아지는 모델에 가까워지도록 하는게 목표

그래서 선형 회귀와 마찬가지로 **경사하강법(Gradient Descent)을 사용하여 모든 데이터에서 로그 손실(Log Loss)을 최소화하는 계수를 찾을 수 있음**

**Classification Threshold (임계값)**

로지스틱 회귀 알고리즘의 결과 값은 ‘분류 확률’

so 확률이 특정 수준 이상 확보되면 샘플이 그 클래스에 속할지 말지 결정이 가능함

대부분의 알고리즘에서 기본 임계 값은 **0.5**

**필요에 따라 모델의 임계값을 변경가능**

예를 들어, 암을 진단하는 로지스틱 회귀 모델을 작성하는 경우에는 혹시 모를 경우에 대비하여 좀 더 민감하게 확인하기 위해 0.3이나 0.4로 임계값을 낮춰 모델의 민감도를 높일 필요가 있다. 그래야 전체적으로 오분류가 더 많아지더라도 실제 암 환자를 놓치는 사례는 적어질 수 잇으니깐

![스크린샷 2023-04-16 오후 6.20.18.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.20.18.png)

임계값 0.5

![스크린샷 2023-04-16 오후 6.20.54.png](Logistic%20Regression,Linear%20Regression%E1%84%87%E1%85%A9%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20c9fabd805a844c1a9e351c8b593ef4be/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.20.54.png)

임계값 0.4

- 로지스틱 회귀 분석은 이진 분류를 수행하는 데 사용됨. 즉, 데이터 샘플을 양성(1) 또는 음성(0) 클래스 둘 중 어디에 속하는지 예측
- 각 속성(feature)들의 계수 log-odds를 구한 후 Sigmoid 함수를 적용하여 실제로 데이터가 해당 클래스에 속할 확률을 0과 1사이의 값으로 나타냄
- 손실함수(Loss Function)는 머신러닝 모델이 얼마나 잘 예측하는지 확인하는 방법이다. 로지스틱 회귀의 손실함수는 Log Loss임
- 데이터가 클래스에 속할지 말지 결정할 확률 컷오프를 Threshold(임계값)이라 한다. 기본 값은 0.5이지만 데이터의 특성이나 상황에 따라 조정할 수 있음.