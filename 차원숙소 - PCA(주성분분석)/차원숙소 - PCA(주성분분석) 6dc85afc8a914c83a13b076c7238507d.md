# 차원숙소 - PCA(주성분분석)

PCA는 대표적인 차원축소에 쓰이는 기법으로 머신러닝,데이터마이닝,통계분석,노이즈 제거 등 다양한 분야에서 널리 쓰이는 새끼

고차원의 데이터를 낮은 차원의 데이터로 바꿀 수 있음…

근데 어떻게 잘 낮추느냐…? 이게 문제임

![스크린샷 2023-05-16 오후 5.04.54.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.04.54.png)

이렇게 2차원을 1차원으로 낮춘다고 할 때

아무리 잘 바꾼다고 하더라도, 2차원의 데이터의 특징을 모두 자세히 존나 잘 살리면서 1차원의 데이터로 바꿔주기는 좀 어렵,,,

그렇다면…?차선책으로 모든 특징을 살릴 수는 없을지라도 최대한 특성을 살리며 차원을 낮춰주는 방법을 고안하기 시작했는데….그게 바로 PCA임!

****PCA 알고리즘의 직관적인 해석****

데이터를 2차원에서 1차원으로 낮춰주는 상황을 가정,

![스크린샷 2023-05-16 오후 5.08.36.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.08.36.png)

자 요런 데이터가 있다고 가정하자 feature은 키,몸무게 이고 2차원임

오른쪽으로 나타내보면 2차원으로 저렇게 표현가능

pca로 1차원으로 낮춰보자…

방법 :

1. 각 축게 대한 평균값을 구한 뒤 해당 점이 원점이 되도록 shift해줌

![스크린샷 2023-05-16 오후 5.09.43.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.09.43.png)

자 x,y축에대한 평균값을 구해준 뒤 각 평균값에 해당하는 점들의 교차점이 원점이 되도록 전체 데이터를 shift해줌

이렇게 몸무게 평균,키 평균을 구해 원점이 있는 저렇게 선들을 만들어 줌

1. 데이터에서 원점을 지나는 직선에 수선의 발을 내려, 해당 길이가 최대가 되는 직선을 찾는다

![스크린샷 2023-05-16 오후 5.16.52.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.16.52.png)

이렇게 모든 데이터에서 원점을 지나는 직선에 수서을 발을 내리면 길이를 구하기 가능

원점에서부터 직선에 내린 수선의 발까지의 길이!!!

그러면 위 그림에서는 빨간선이 제일 최대의 길이임!!

근데, 원점을 지나는 직선의 기울기가 변함에 따라, 빨간선들의 길이 또한 변하게 됨. PCA에선 이 빨간선 즉 최대길이 제곱들의 합이 최대가 되는 직선을 찾는다(sum of squares(ss))라고 부름

![스크린샷 2023-05-16 오후 5.27.23.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.27.23.png)

길이 합이 최대가 되는 최적의 직선을 찾다보면 초록색 직선이 됨

1. 찾은 직선을 PC1으로 설정하고, loading score을 구함
    
    ![스크린샷 2023-05-16 오후 5.28.10.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.28.10.png)
    

이때, PC1과 방향이 같은 벡터를

**pc1의 singular vector** or **pc1의 eigenvector**라고 함

또한 pc1의 singular vector의 x,y축 길이의 비율을 loading score라고 함

위 그림에서는 0.97,0.242만큼의 비율이 바로 loading score

1. pc1에 직교하는 직선을 pc2로 잡음

![스크린샷 2023-05-16 오후 5.40.57.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.40.57.png)

위 경우 2차원이므로 pc1에 직교하는 직선이 유일하지만, 만약 3차원의 경우는 pc1에 직교하는 직선이 평면으로 나올 것

이 경우에는 pc1에 직교하는 평면중에서 위의 2번의 과정을 다시 거쳐 수선의 발까지의 거리의 합이 최대가 되는 직선을 선택해주면 됨!

참고로 N 차원 데이터에는 N 개의 PC 직선이 나오게 된다고 함

1. pc1,pc2를 축으로 하여 회전시킨뒤 scree plot생성

![스크린샷 2023-05-16 오후 5.42.54.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.42.54.png)

이제 **scree plot**을 생성해줘야 하는데, scree plot은 각 PC 축이 전체 데이터를 얼마나 잘 표현하고 있는지를 나타내주는 역할을 함

scree plot 을 생성하기 위해, PC1과 PC2의 SS 비율을 구해야 함

만약 PC1과 PC2의 SS 비율이 8.9 : 1.1 이라고 하면

![스크린샷 2023-05-16 오후 5.48.54.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.48.54.png)

이렇게 그래프로 나타낼 수 있는데…

의미는 pc1축이 전체 데이터 특징의 89퍼, pc2축이 전체 데이터 특징의 11퍼정도를 나타내고 있다는 뜻

만약 내가 89퍼 정도의 특징만으로도 해당 데이터를 잘 나타낼 수 있을 것이라고 판단되면,pc1만가지고 1차원 나타낼 수 있음

![스크린샷 2023-05-16 오후 6.11.32.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.11.32.png)

그렇다면 뭐 이런식으로 되겟지

다른 예를 들어, 3차원 데이터가 있는데, PCA를 통해 PC1 PC2 PC3 3개의 축을 잡았고, scree plot 을 그려본 결과 PC1이 73%, PC2가 17%, PC3가 10% 를 차지한다고 해보자. 이러면 개발자에게는 차원을 줄이는 두가지 선택권이 있는 것

**1)** 데이터 특징의 90% 를 살리며 3차원에서 2차원으로 차원을 축소하는 선택권 (PC1, PC2 선택)

**2)** 데이터 특징의 73% 를 살리며 3차원에서 1차원으로 차원을 축소하는 선택권 (PC1만 선택)

무엇을 선택할지는 현재 시스템의 리소스 상황을 고려하며 선택하면 된다

**3. PCA 알고리즘의 수학적인 해석**

그 전에 알아두어야 할 것이 covariance matrix, Eigenvector

**Background1) Variance**

분산, 데이터가 얼마나 넓게 퍼져있냐…?

분산은 편차의 제곱합의 평균으로 구할 수 잇음

![스크린샷 2023-05-16 오후 6.18.11.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.18.11.png)

평균이 0인 1차원 데이터를 나타낸 그림인데 위의 데이터의 variance는

(1+0+1)/3 = 2/3이고, 아래 데이터의 variance는 (9+0+9)/3 = 6

**Background2) covariance**

이거는 고차원에서의 데이터들 간의 분산을 나타내는 값

위에는 1차원에서의 분산인데, 2차원에서는 어케할까?

x축에서의 variance와 y축에서의 variance를 융합하면 됨

뭔 쌉소리일까?

![스크린샷 2023-05-16 오후 6.20.45.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.20.45.png)

이런 데이터가 있다고 가정할 때, x,y축에서의 분산이 같을 것이므로 더한 값도 값을 것

하지만 위의 두 데이터는 완전히 다른 분포를 나타내는 데이터이지만, covariance가 같아지므로, 단순히 더하는 방법은 옳지 않은 방법

---

 covariance?

>>>

통계 및 확률 이론에서 공분산은 두 변수가 함께 어떻게 변하거나 달라지는지를 측정한 것 두 확률 변수 사이의 관계를 정량화합니다. 특히 공분산은 한 변수의 변화가 다른 변수의 변화와 연관되는 정도를 측정함

수학적으로 두 변수 X와 Y 사이의 공분산은 각 변수와 해당 평균 간의 차이 곱의 평균을 취하여 계산.

cov(X, Y) = E[(X - E[X])(Y - E[Y])]

여기서 E[X]와 E[Y]는 각각 변수 X와 Y의 평균을 나타냅니다. 결과는 양수, 음수 또는 0이 될 수 있는 단일 숫자

양의 공분산은 변수 간의 직접적인 관계를 나타냄

즉, 한 변수가 증가하면 다른 변수도 증가하는 경향이 있음

음의 공분산은 한 변수의 증가가 다른 변수의 감소에 해당하는 역관계를 나타냄 

공분산이 0이면 변수 간에 선형 관계가 없음을 나타냄

---

이런 문제를 해결하기 위해 covariance를 구하는 방식은 x값과 y값을 각각 x, y값의 평균의 차 곱하여 더하고(= 내적) n으로 나눠줌으로써 구할 수 있음

![스크린샷 2023-05-16 오후 6.59.25.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.59.25.png)

**※ 참고 ※**

covariance를 구하기 위해 내적하는 과정은 데이터의 평균값이 0 일때만 유효하다. 2차원 데이터일 경우, 데이터의 x축 평균과 y축 평균이 모두 0이어야 하며, 0이 아닐 경우 각각 평균값을 빼주면 된다. (앞서 직관적인 해석에서 본 STEP 1) 과정과 일치)

**Background3) covariance matrix**

covariacne 값을 통해 covariance matrix 라는 행렬을 만들어 낼 수 있음

![스크린샷 2023-05-16 오후 7.07.00.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_7.07.00.png)

![스크린샷 2023-05-16 오후 7.08.39.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_7.08.39.png)

예를들어 위와 같은 공분산행렬이 있다고 할 때

x의 분산은 y의 분산보다 크므로 데이터는 가로로 길쭉한 형태가 될 것

x의 variance는 y의 variance보다 크므로 데이터는 가로로 길쭉한 형태가 될 것이며, x와 y의 covariance가 양수이므로 1, 3 사분면을 지나는 오른쪽 그림과 같은 형태가 될 것

선형변환 관점에서 바라본다면…?

![스크린샷 2023-05-16 오후 7.10.37.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_7.10.37.png)

원점을 기준으로 골고루 퍼져있는 normal distributied 데이터에 covariance matrix를 곱해서 선형변환을 수행해주면, 아까 봤던 것처럼 covariance matrix의 특성에 맞게 데이터가 쭉 늘어나는 형태로 변하는데, 이렇게 늘어나는 것을 shearing 이라고 함

**Background4) Eigenvector, Eigenvalue**

eigenvector(고유벡터)라는 새끼는 행렬A에 의해 선형변환되는 수많은 벡터들 중에, 변환되기 전과 변환된 후의 벡터 방향이 똑같은 벡터!

![스크린샷 2023-05-16 오후 7.12.49.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_7.12.49.png)

저 중간 행렬을 갈기면, 오른쪽 같은 그림이 되는데 존나 만흔 벡터들 중에 빨간색 표시된 벡터는 방향이 동일!

빨간 두개의 벡터가 해당 선형 시스템 방정식에 대응는 **Eigenvector**

이때 Eigenvecotr의 변환 전과 후의 길이 변화 비율을 **Eigenvalue**

![스크린샷 2023-05-16 오후 7.14.16.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_7.14.16.png)

수식으로 보면, 선형 변환이 되기 전과 후의 방향이 같은 벡터 v가

Eigenvector이며, 변화되는 길이의 비율 λ값이 Eigenvalue임

N 데이터에서는 N개의 Eigenvector/Eigenvalue가 나옴

위의 예시에서는 2차원 데이터였으므로 2개의 Eigenvector/Eigenvalue가 나온 것

자 이 ㅈ같은 개념들을 가지고 수학적으로 해석해보자 PCA를 말이야

1. 고차원의 데이터의 공분산 행렬을 구함
2. 구한 공분산 행렬에서 Eigenstuff(Eigenvector, Eignevalue)를 구함
3. 구한 Eigenstuff를 Eigenvalue가 큰것부터 작은 순서대로 정렬

앞서 직관적인 해석에서 알아보았던 PC1, PC2, ... 축 들이 바로 여기서 구한 Eigenvector들

앞서 직관적인 해석에서 알아보았던 scree plot에서 각 PC들이 가졌던 비율들이, 바로 여기서 구한 Eigenvalue 값들의 비율

즉, Eigenvalue가 큰 것부터 작은 순서대로 정렬한 것이 일종의 scree plot을 그린 것이라고 보면 된다.

1. 원하는 만큼 Eigenvector를 쳐냄으로써 차원 축소를 해줌

![스크린샷 2023-05-16 오후 7.18.14.png](%E1%84%8E%E1%85%A1%E1%84%8B%E1%85%AF%E1%86%AB%E1%84%89%E1%85%AE%E1%86%A8%E1%84%89%E1%85%A9%20-%20PCA(%E1%84%8C%E1%85%AE%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8)%206dc85afc8a914c83a13b076c7238507d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-05-16_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_7.18.14.png)

만약 5차원 데이터에서 eigenstuff를 구한 뒤 eigenvalue를 기준으로 정렬했다고 하면 위 그림처럼 나오는데

여기서 2차원으로 줄이겠다라고 하면 큰 두개를 빼고 나머지 3개를 쳐내면 됨

그러면 두개의 Eigenvector를 축으로 하는 2차원 데이터로 차원을 줄일 수 있으며, 해당 2차원 데이터가 나타내는 데이터의 특징은, 두 Eigenvector v1, v2 의 Eigenvalue가 가지는 비율인 86%

정릴하자면 

**1) N차원의 데이터로부터 Covariance matrix를 생성한다.**

**2) 생성된 covariance matrix에서 N개의 Eigenvector, Eigenvalue를 찾는다.**

**3) 찾은 Eigenvector를 Eigenvalue가 큰 순서대로 정렬한다.**

**4) 줄이기 원하는 차원 개수만큼의 Eigenvector만 남기고 나머지는 쳐낸다.**

**5) 남은 Eigenvector를 축으로 하여, 데이터의 차원을 줄인다.**