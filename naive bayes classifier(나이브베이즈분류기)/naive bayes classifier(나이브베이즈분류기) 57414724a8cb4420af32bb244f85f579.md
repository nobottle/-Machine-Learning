# naive bayes classifier(나이브베이즈분류기)

**optimal classification**

bayes classifier

f= argmin , P(f(x) =/ Y

P(f(x) = y^(와이 헷) >>y가 아닌 추정치

즉 Y랑 y^이랑 같이 않을 확률을 최소화하는 f를 찾아내서 f*라고 부르겠따

![스크린샷 2023-04-07 오후 11.36.37.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.36.37.png)

그러면 이거를 최소화하는 함수를 만들어야 함

그러면? 어떠한 x를 넣었을 때 원하는 Y가 나오도록 확률밀도함수를 최대로 만들어 주는 함수를 구해야 함

MLE : 관측된 값만 가지고

![스크린샷 2023-04-07 오후 11.41.00.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.41.00.png)

h,t랑 나온 값만 가지고 세타헷을 만들엇음

MAP : 사전정보가 주어짐

![스크린샷 2023-04-07 오후 11.41.52.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.41.52.png)

a,B가 추가정보 사전정보

그럼 이 두개를 가지고 어케 좋은 classification을 만들 수 있을까?

![스크린샷 2023-04-07 오후 11.44.36.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.44.36.png)

![스크린샷 2023-04-07 오후 11.45.04.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.45.04.png)

P1+P2는 1

실선에서도 마찬가지

![스크린샷 2023-04-07 오후 11.46.16.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.46.16.png)

Xm에서의 y값은? 50대 50

빨,초가 마나는 지점이여서 중간이기 때문

so 0.5라고 정의가능 

근데…? 실선,점선중에 어디가 더 나을까?

why?

![스크린샷 2023-04-07 오후 11.47.45.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.47.45.png)

저 가운데 xn지점을 봣을 때 

점선의 네모

실선 동그라미

y값이 있을 때 점선(선형함수)의 거리가 더 가깝다

이 말은 즉슨 ? 선형모델은 ?빨,초가 될지 어디가 될지 높은 확률로 구분을 못해줌 왜냐 거리가 가깝기 때문 거리가 멀어야 차이를 명확하게 보여줄 수 있기 때문

xn은 Decision boundary

점선으로 클래스를 구분하면?

왼쪽을 봤을 때 x값들은 초록색들의 확률이 높다고 판단할 것

그렇다면? 빨간색 부분만큼은 error다 (존재하는 확률이잖아? 빨간색은 분명히)

![스크린샷 2023-04-07 오후 11.52.34.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.52.34.png)

그렇다면 실선으로 하면?

![스크린샷 2023-04-07 오후 11.52.53.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.52.53.png)

오 점선보다 낮게 차있다 즉 error의 크기가 점선보다 작음!!!

![스크린샷 2023-04-07 오후 11.53.46.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.53.46.png)

여기가 바로 점선과 실선사이의 성능차이!

이 리스크를 줄이는 함수를 줄이는 함수를 만들어야 하는데

줄여야 하는데 베이즈 영역(반달모양 부분)을…

optimal classifier

![스크린샷 2023-04-07 오후 11.57.29.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_11.57.29.png)

데이터로 주어졌을 때 나오는 y와 우리가 판별해서 나오는 Y가 같은 값이 되도록 만들어주는 확률을 만들어주는 함수

prior = class prior = P(Y=y)

likelihood = class conditional density = P(X=x|Y=y) 

베이즈 정리

두 사건 x,y에 대한 조건부 확률간에 성립하는 확률 관계

![스크린샷 2023-04-08 오전 3.13.40.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_3.13.40.png)

예를들어

P(A|B)라는 확률은 사건 B가 일어났다는 가정 하에 사건 A가 발생할 확률 사건 A, B에 대한 조건부 확률 수식은
 

![스크린샷 2023-04-08 오전 3.14.32.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_3.14.32.png)

베이즈 정리 수식은 확률의 곱 규칙과 확률의 합 규칙으로부터 유도

![스크린샷 2023-04-08 오전 3.15.06.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_3.15.06.png)

확률의 곱 규칙은 두 사건 x,y가 동시에 일어날 확률

확률의 합 규칙은 x,y중 하나만 일어날 확률

**P(Y|X) : 사후확률(Posterior)을 뜻한다**

**사후확률이란 사건 발생 후의 확률을 의미하는데, 정확히 말하자면 발생한 사건(X)이 특정 확률분포(Y)에서 나왔을 확률** 

**머신러닝 관점에서는 관측된 특징(X)이 특정 클래스(Y)에서 나왔을 확률**

ex)뚱뚱한데(x) 못생긴사람(y) 뚱뚱한 사람중에 못생긴 사람이 나왓을 확률

**P(X|Y) : 우도 또는 가능도(Likelihood)를 뜻함**

**우도란 사후확률과 반대로, 특정 확률분포 or 클래스(Y)에서 특정 사건 or 특징(X)이 발생할 확률을 뜻한다**

**머신러닝 관점에서는 기존에 있는 데이터의 각 클래스 별로 특정 특징에 대한 분포를 의미**

ex)못생긴 사람중에서 뚱뚱한 사람이 나올 확률 posterior와 반대로 생각

**P(Y) : 사전확률(Prior)를 뜻** 

**특정 특징 or 사건에 무관하게 미리 알 수 있는 확률**

**머신러닝 관점에서는 특징(X)가 관측되기 전부터 이미 정해져있던 클래스(Y)의 분포를 의미**

ex)뚱뚱한 사람들이 관측되기 전에 이미 정해진 못생긴 사람들

**P(X) : 증거(Evidence)를 뜻**

**머신러닝 관점에서는 특정 클래스(Y)의 분포에 상관없이 전체 클래스에서 특정 특징(X)이 관측될 확률을 뜻**

ex) 못생긴 사람들 상관없이 뚱뚱한 애들이 관측될 확률

**MAP(최대 사후 확률 추정)**

![스크린샷 2023-04-08 오전 3.26.05.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_3.26.05.png)

x를 뽑았는데 y가 나오는데 최대가 나오는 y헷을 찾는거

ex) 

반 = y

이쁜애 = x

반이 10개 잇다고 치면

이쁜애를 뽑았는데 걔가 속해있는 반이 어느 몇반에서 나왓는지 추정을 하는 거

**MLE(최대 우도 측정)**

![스크린샷 2023-04-08 오전 3.30.34.png](naive%20bayes%20classifier(%E1%84%82%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B3%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%80%E1%85%B5)%2057414724a8cb4420af32bb244f85f579/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_3.30.34.png)

특정 데이터 분포를 나타내는 모수를 Y라고 할 때, 해당 모수에서 만들어진 데이터 분포로부터 특징 X가 발견될 확률인 우도 P(X|Y)를 최대화하는 Y인 Y헷

특징 x가 주어졌을 때 x를 발생시켯을 가능성을 최대로 하는 모수를 찾아라!!

우도를 최대화하는 확률 모형을 만듦으로써 특정 클래스에 대한 특징 데이터의 분포를 더욱 정확하게 알 수 잇을듯

**최대 사후확률 추정은 최대 우도 추정과 다르게 사전확률까지 사용해서 어떤 데이터의 분포를 나타내는 모수 Y를 추론하므로 좀 더 정확하다는게 장점 반면 사전확률의 영향을 많이 받고, 그 사전확률의 분포를 알아내는 것도 현실에선 어렵다는 것이 단점**

MLE는 우도함수를 최대화하여 매개변수를 추정하는 방법이고, MAP는 매개변수에 대한 사전지식을 고려하여 베이즈 정리를 이용하여 매개변수를 추정하는 방법

MLE는 관찰된 데이터의 우도만 고려하는 반면 MAP는 매개변수의 우도 및 사전 확률 분포를 모두 고려