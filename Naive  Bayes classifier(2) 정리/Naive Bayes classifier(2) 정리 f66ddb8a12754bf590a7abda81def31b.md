# Naive  Bayes classifier(2) 정리

**Naive  Bayes classifier**

Naive Bayes 분류기는 기계 학습의 분류 문제에 사용되는 간단하고 효과적인 알고리즘

ex)

일부는 스팸이고 일부는 스팸이 아닌 일련의 이메일이 잇을때,  새 이메일을 스팸으로 분류하는 할 때, Naive Bayes 분류기는 이메일에서 특정 단어의 빈도를 분석하고 해당 정보를 사용하여 예측하는 방식으로 작동

**Bag of Words**

텍스트 데이터를 단어 집합으로 표현하는 데 사용되는 자연어 처리 기술로, 문법과 단어 순서는 무시하지만 발생 빈도를 추적

텍스트 모음에서 단어 어휘를 만든 다음 각 문서를 단어 빈도의 벡터로 나타내는 작업

---

**1.베이즈 추정**

추론 대상의 사전 확률과 추가적인 정보를 기반으로 해당 대상의 사후 확률을 추론하는 통계적 방법

![스크린샷 2023-04-12 오후 4.39.53.png](Naive%20Bayes%20classifier(2)%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20f66ddb8a12754bf590a7abda81def31b/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-12_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.39.53.png)

계산식

(참조)

[https://bkshin.tistory.com/entry/dd?category=1042793](https://bkshin.tistory.com/entry/dd?category=1042793)

데이터를 모델에 잘 학습시키면 어떠한 새로운 데이터를 줘도 ex)개,고양이 인지 정확히 분류함

이거는 사전 데이터들로 학습을 오지게 잘 시키면 가능

이렇듯 사전 데이터를 기반으로 학습 오지게 시키는 방법을 supervised learning이라고 함

지도학습을 하기 위한 첫 스텝은 Feature와 Label을 파악하는 것

label : 분류결과 예를들어 개,고양이 같은 클래스

feature : 결과에 영향을 주는 요소들 뭐 털,와꾸,모양새 등등

즉 feature를 기반으로 label분류를 함

나이브 베이즈 분류는 지도학습의 일종, 따라서 Feature와 Label이 필요

feature에 따라 Label을 분류하는데 베이즈 정리를 사용하는 것이 특징, and feature가 서로 indepent해야한다는 가정 필요

![스크린샷 2023-04-12 오후 4.53.07.png](Naive%20Bayes%20classifier(2)%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20f66ddb8a12754bf590a7abda81def31b/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-12_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.53.07.png)

1.주어진 데이터로 train시킴 특정 feature를 기반으로 label파악

스팸 메일을 예로 들면 스팸 메일인지 아닌지의 여부가 Label

이 Label 결과에 영항을 주는 요소가 Feature, 스팸 메일의 제목 및 내용에 기재된 광고성 단어, 비속어, 성적 용어등이 하나하나의 feature

Label -> 스팸 메일인 경우 Label = 1, 스팸 메일이 아닌 경우 0 뭐 이런식으로 가능

근데 Naive Bayes Classifier는 feature끼리 독립적이라는 조건이 필요하다고 햇는데, 즉 스펨 메일 분류에서 광고성 단어의 개수와 비속어의 개수가 서로 연관이 있어서는 안 됨

![스크린샷 2023-04-12 오후 5.05.15.png](Naive%20Bayes%20classifier(2)%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20f66ddb8a12754bf590a7abda81def31b/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-12_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.05.15.png)

맨 왼쪽 : 날씨에 따라 축구를 했는지 안했는지에 대한 과거 데이터

Frequency Table : 주어진 과거 데이터를 횟수로 표현한 것

Likelihood Table 1 : Feature (여기서는 날씨)에 대한 확률, 각 Label (여기서는 축구를 할지 말지 여부)에 대한 확률

. Likelihood Table 2 : 각 Feature에 대한 사후 확률

### 문제 1. 날씨가 overcast일 때 경기를 할 확률은?

P(Yes|Overcast) = P(Overcast|Yes) P(Yes) / P(Overcast)   <- 베이즈 정리에 의해 이런 식이 나옴

**1. 사전 확률**

P(Overcast) = 4/14 = 0.29

P(Yes) = 9/14 = 0.64

**2. 사후 확률**

P(Overcast|Yes) = 4/9 = 0.44

**3. 베이즈 정리 공식에 대입**

P(Yes|Overcast) = P(Overcast|Yes) P(Yes) / P(Overcast) = 0.44 * 0.64 / 0.29 = 0.98

즉, 날씨가 Overcast일 때 축구를 할 확률이 0.98이라는 뜻

### 문제 2. 날씨가 Overcast일 때 경기를 하지 않을 확률은?

P(No|Overcast) = P(Overcast|No) P(No) / P(Overcast)

**1. 사전 확률**

P(Overcast) = 4/14 = 0.29

P(No) = 5/14 = 0.36

**2. 사후 확률**

P(Overcast|No) = 0/5 = 0

**3. 베이즈 정리 공식에 대입**

P(No|Overcast) = P(Overcast|No) P(No) / P(Overcast) = 0 * 0.36 / 0.29 = 0

즉, 날씨가 Overcast일 때 축구를 할 확률이 0이라는 뜻

P(Yes|Overcast) = 0.98, P(No|Overcast) = 0 즉, 날씨가 Overcast일 때 축구를 하는 확률은 0.98, 축구를 하지 않을 확률은 0. 두 확률을 비교한 뒤 더 높은 확률의 Label로 분류를 하면 됩니다. 두 확률을 비교했을 때 'Yes' Label의 확률이 0.98로 더 높다  따라서 나이브 베이즈 분류기는 날씨가 Overcast일 때 축구를 할 것이라고 판단

나이브 베이즈는 베이즈 정리를 활용하여 확률이 더 큰 Label로 분류